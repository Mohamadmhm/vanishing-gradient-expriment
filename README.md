# vanishing-gradient-expriment

Using Sigmoid as an active function decreases the weights in the first layers when the number of layers increases, but using ReLU can prevent that most of the time.
